# -*- coding: utf-8 -*-
"""helpdesk.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rfCtm_HWarsds3TrYGRpbzkqRdqhj9w-
"""

import pandas as pd
import numpy as np
import keras
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense, Embedding, LSTM, Dropout, Activation, Flatten
from sklearn.model_selection import train_test_split
from keras.utils.np_utils import to_categorical
from keras.layers.normalization import BatchNormalization
import sklearn
from sklearn.utils import shuffle
from datetime import datetime
from datetime import timedelta
from sklearn.preprocessing import MinMaxScaler,StandardScaler
from scipy import stats
import matplotlib.pyplot as plt
from IPython.display import clear_output
from keras.models import load_model
from keras.models import Model
from keras.layers import *
import datetime

#import the csv
data = pd.read_csv('helpdesk.csv')

#given the caseID, array 'numarray' outputs the number of events in  that particular caseID trace.
caseS = pd.Series(data['CaseID'] )
numarray = caseS.value_counts()#this numarray will be used to iterate the panda(data) as it contains the number of events per trace
numcaseID = pd.value_counts(data['CaseID']).value_counts()
maxlen = max(numcaseID.index)#maxlen is the maximum number of events in a trace that was observed
actS = pd.Series(data['ActivityID'] )
actS.value_counts()
classes = actS.max()
print("number of classes(zero not included which will be denoted later as end of trace) :" +str(classes))

#converting it into datetime datatype of pandas for easy access of date, time and other arithematic operations.
data['datetime'] = pd.to_datetime(data['CompleteTimestamp'])
data.drop(['CompleteTimestamp'], axis=1, inplace=True)

#set finds the unique numbers in caseID i.e list all the unique caseIDs.
#datetime is shifted using shift command to find elapsed
# or concerned type of time further down the line.
case_uniq = list(set(data['CaseID'].values))
data['shiftedtime'] = data['datetime'].shift(1)

#difference of datetime is timedelta type in pandas which has a function called .total_seconds()
data['elapsed'] = data['datetime'] - data['shiftedtime']
elapsec = [x.total_seconds() for x in data['elapsed'] ]
data['elap'] = elapsec
data.drop(columns=['elapsed','shiftedtime'],inplace = True)
#at this point of time, the columns are caseID, activityID, timestamp and elapsed time

#for below loop, case at all times denote the index of data which is a panda, 
#and case_uniq contains unique caseIDs, 
#we increment case with numarray[i] amount i.e the number of events 
#in that particular trace denoted by i (element of case_uniq).
case = 0
for i in case_uniq:
  data.loc[case,'elap'] = 0
  case += numarray[ i ]

#converted the seconds into days
lis = [(x.hour*3600 + x.minute*60 + x.second) for x in data['datetime']]
data['from_midnight']  =lis

#with the help of data['from_midnight'].hist() 
#we notice by below graphs that most stuff happened during the evening/night
#we find the from_midnight and from_sun attributes using .weekday 
#function of datetime object and converted everything into time(days) scale.
li = [x.weekday()*86400 for x in data['datetime']]
data['from_sun'] = li
data['from_sun'] = (data['from_sun'] +  data['from_midnight'])/86400
data['from_midnight'] = (data['from_midnight'])/86400
data['elap'] = (data['elap'])/86400

#we see that no event occurred on Sunday and least events happened on Tuesday, most on Monday.
print("Below is the compilation of information of all the data we have conjured.")
print(data.describe())

# with the help of np.sum(data.isna())
# we see if any null values are there, the results were zero in all the columns 
#the result was zero

#gcid denotes the group number categorized according to the caseID or the trace number.
gcid = data.groupby('CaseID')

#categorizing the data by individual traces
act = []#contains all the events of a particular trace as one element.
time = []#contains timestamp of all events of a particular trace as one element
ofea = []#contains ofea(other features) like elapsed time, time from midnight, time from last sunday of all traces as one element
for cid in case_uniq:
  temp = gcid.get_group(cid)
  act.append(temp['ActivityID'].values)
  time.append(temp['datetime'].values)
  ofea.append(temp[['elap','from_midnight','from_sun']].values)

#padding the above arrays with the maximum number of events possible
# in our dataset which is helpdesk maximum of 14 element was found in a trace
#the padding is done so that operations could be easily performed
#we calculated maxlen earliar, it gave 14 as answer for this dataset
padded_act = pad_sequences(act, padding = "post", maxlen = maxlen)
padded_time = pad_sequences(time, padding = "post", maxlen = maxlen, dtype = 'datetime64[ns]')
padded_ofea = pad_sequences(ofea, padding = "post", maxlen = maxlen, dtype = 'float64')

#finding one-hot encoding of the padded activity for each concatenation and access later.
X = to_categorical(
    padded_act,
    num_classes= classes+1,
    dtype = 'int'
)

#concatinating other features(ofea) into the one-hot encoding making the shape(-1,14,13) for this dataset.
X = np.concatenate((X,padded_ofea) , axis = 2)

"""**PREFIX = decide here**"""
prefix = 2

#We remove all the traces where the end of trace has already occurred as they would make our model bias if taken
# we also keep the corresponding timestamps in time array for further usage down the line 
X_train = X[X[:,prefix-1,0] == 0]
time = padded_time[X[:,prefix-1,0] == 0]
print("The shape of the data before cutting it for training and validation: "+str(X_train.shape))

#For our purposes we take 80% of data for training and rest for testing results.  
till = int(X_train.shape[0] *0.8)
X_test = X_train[till:]
X_train = X_train[:till]
print("The shape of the training data "+str(X_train.shape))
print("The shape of the testing data "+str(X_test.shape))

#we do this for time array previously formed as well
time_test = time[till:]
time_train = time[:till]

nb_epoch = 50
batch_size = 16#since number of traces are small, we take small batch_size
hidden_units = 100
num_features = 13

class PlotLosses(keras.callbacks.Callback):
  
    '''class to implement callback in keras
    it would graph the losses as the model trains'''
    def on_train_begin(self, logs={}):
        self.i = 0
        self.x = []
        self.losses = []
        self.val_losses = []
        
        self.fig = plt.figure()
        
        self.logs = []

    def on_epoch_end(self, epoch, logs={}):
        
        self.logs.append(logs)
        self.x.append(self.i)
        self.losses.append(logs.get('loss'))
        self.val_losses.append(logs.get('val_loss'))
        self.i += 1
        
        clear_output(wait=True)
        plt.plot(self.x, self.losses, label="loss")
        plt.plot(self.x, self.val_losses, label="val_loss")
        plt.xlabel('epochs')
        plt.ylabel('loss in MAE')
        plt.legend()
        plt.show();
        
plot = PlotLosses()

"""**activity prediciton-no shared layer**"""

#model1 is separate model for only activity prediction, hence no shared layer
model1 = Sequential()
model1.add(LSTM(hidden_units, input_shape = (prefix,num_features),return_sequences=True))
model1.add(Dropout(0.2))
model1.add(LSTM( 100))
model1.add(BatchNormalization())
model1.add(Dropout(0.2))
model1.add(Dense(classes+1,activation='softmax'))

print(model1.summary())
model1.compile(loss = 'mean_squared_error', optimizer='adam',metrics = ['accuracy'])
print("No shared layer activity prediction")
history = model1.fit(X_train[:,:prefix,:], X_train[:,prefix,:10], epochs=15, batch_size=batch_size,validation_split=0.2,callbacks=[plot]
          ,verbose=0)

accr = model1.evaluate(X_test[:,:prefix,:],X_test[:,prefix,:10])
print('No shared layer activity prediction :Test set\n  Loss: {:0.3f}\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))

"""**Time prediction - no shared layer**"""

#model2 calcualtes time only hence no shared layer
model2 = Sequential()
model2.add(LSTM(hidden_units,input_shape = (prefix,num_features),return_sequences=True))
model2.add(Dropout(0.5))
model2.add(LSTM(100 ))
model2.add(BatchNormalization())
model2.add(Activation('relu'))
model2.add(Dropout(0.5))
model2.add(Dense(128,activation='relu'))
model2.add(Dense(32,activation='relu'))
model2.add(Dense(1,activation='linear'))
print(model2.summary())
model2.compile(loss = 'mean_squared_error', optimizer='adam',metrics = ['mae'])
print("No shared layer time prediction")
history = model2.fit(X_train[:,:prefix,:], X_train[:,prefix,10], epochs=50, batch_size=batch_size,validation_split=0.2,callbacks=[plot]
          ,verbose=0)

accr = model2.evaluate(X_test[:,:prefix,:],X_test[:,prefix,10])
print('No shared layer activity prediction Test set\n  Loss: {:0.3f}\n  MAE: {:0.3f}'.format(accr[0],accr[1]))

"""**shared layer of activity and time**"""

#shared model where first layer is common then two layers branch out specializing in activity and time prediction respectively
inp = Input((prefix,num_features),name='inp')  

x = LSTM(hidden_units,input_shape = (prefix,num_features),return_sequences=True)(inp)
x = Dropout(0.4)(x)
temp1 =LSTM(100 )(x)
temp = LSTM(100)(x)

temp = BatchNormalization()(temp)
temp = Activation('relu')(temp)
temp= Dropout(0.4)(temp)
temp1= Dropout(0.4)(temp1)
temp = Dense(128,activation='relu')(temp)
temp = BatchNormalization()(temp)
temp = Activation('relu')(temp)
temp= Dropout(0.4)(temp)

act_output = Dense(classes+1, activation='softmax', kernel_initializer='glorot_uniform', name='act_output')(temp1)
time_output = Dense(1, kernel_initializer='glorot_uniform', name='time_output')(temp)


model3 = Model(inputs=[inp], outputs=[act_output, time_output])
print(model3.summary())
model3.compile(loss={'act_output':'categorical_crossentropy', 'time_output':'mean_squared_error'}, optimizer='Adam',metrics = {'act_output':'accuracy', 'time_output':'mae'})
print("1 shared layer predictions")
history = model3.fit(X_train[:,:prefix,:], {'act_output':X_train[:,prefix,:10], 'time_output':X_train[:,prefix,10]}, epochs=50, batch_size=batch_size,validation_split=0.2,callbacks=[plot]
          ,verbose=0)

accr = model3.evaluate(X_test[:,:prefix,:],{'act_output':X_test[:,prefix,:10], 'time_output':X_test[:,prefix,10]})
print('1 shared layer: Test set\n  Loss: {:0.3f}\n  act_output_acc: {:0.3f} \n  time_output_mean_absolute_error: {:0.3f}'.format(accr[0],accr[3],accr[4]))


"""**Attention layer implementation** """

class Attention(Layer):
    '''attention layer which inputs a layer 
    and according to the shape of the current 
    input, outputs augmented layer '''
    def __init__(self, nb_head, size_per_head, **kwargs):
        self.nb_head = nb_head
        self.size_per_head = size_per_head
        self.output_dim = nb_head*size_per_head
        super(Attention, self).__init__(**kwargs)

    def build(self, input_shape):
        self.WQ = self.add_weight(name='WQ', 
                                  shape=(input_shape[0][-1], self.output_dim),
                                  initializer='glorot_uniform',
                                  trainable=True)
        self.WK = self.add_weight(name='WK', 
                                  shape=(input_shape[1][-1], self.output_dim),
                                  initializer='glorot_uniform',
                                  trainable=True)
        self.WV = self.add_weight(name='WV', 
                                  shape=(input_shape[2][-1], self.output_dim),
                                  initializer='glorot_uniform',
                                  trainable=True)
        super(Attention, self).build(input_shape)
    def call(self, x):
        if len(x) == 3:
            Q_seq,K_seq,V_seq = x
            Q_len,V_len = None,None
        Q_seq = K.dot(Q_seq, self.WQ)
        Q_seq = K.reshape(Q_seq, (-1, K.shape(Q_seq)[1], self.nb_head, self.size_per_head))
        Q_seq = K.permute_dimensions(Q_seq, (0,2,1,3))
        K_seq = K.dot(K_seq, self.WK)
        K_seq = K.reshape(K_seq, (-1, K.shape(K_seq)[1], self.nb_head, self.size_per_head))
        K_seq = K.permute_dimensions(K_seq, (0,2,1,3))
        V_seq = K.dot(V_seq, self.WV)
        V_seq = K.reshape(V_seq, (-1, K.shape(V_seq)[1], self.nb_head, self.size_per_head))
        V_seq = K.permute_dimensions(V_seq, (0,2,1,3))
        A = K.batch_dot(Q_seq, K_seq, axes=[3,3]) / self.size_per_head**0.5
        A = K.permute_dimensions(A, (0,3,2,1))
        A = K.permute_dimensions(A, (0,3,2,1))    
        A = K.softmax(A)
        O_seq = K.batch_dot(A, V_seq, axes=[3,2])
        O_seq = K.permute_dimensions(O_seq, (0,2,1,3))
        O_seq = K.reshape(O_seq, (-1, K.shape(O_seq)[1], self.output_dim))
        return O_seq
      
    def compute_output_shape(self, input_shape):
        return (input_shape[0][0], input_shape[0][1], self.output_dim)

#attention model where first two layers are common namely LSTM and attention
# then two layers branch out specializing in activity and time prediction respectively
inp = Input((prefix,num_features),name='inp')  
x = LSTM(hidden_units,return_sequences=True)(inp)
temp= Dropout(0.5)(temp)
temp = Attention(2, 64)([x, x, x])  #output: [batch_size, time_step, nb_head*size_per_head]
temp = BatchNormalization()(temp)
temp = Activation('relu')(temp)
temp= Dropout(0.5)(temp)
temp = Dense(128,activation='relu')(temp)
temp= Dropout(0.5)(temp)
temp1 = Dense(128,activation='relu')(temp)
temp = Flatten()(temp)
temp1 = Flatten()(temp1)

act_output = Dense(classes+1, activation='softmax', kernel_initializer='glorot_uniform', name='act_output')(temp1)
time_output = Dense(1, kernel_initializer='glorot_uniform', name='time_output')(temp)


attmodel = Model(inputs=[inp], outputs=[act_output, time_output])
print(attmodel.summary())
attmodel.compile(loss={'act_output':'categorical_crossentropy', 'time_output':'mean_squared_error'}, optimizer='Adam',metrics = {'act_output':'accuracy', 'time_output':'mae'})
print("attention added model predictions")
history = attmodel.fit(X_train[:,:prefix,:], {'act_output':X_train[:,prefix,:10], 'time_output':X_train[:,prefix,10]}, epochs=5, batch_size=batch_size,validation_split=0.2,callbacks=[plot]
          ,verbose=0)

accr = attmodel.evaluate(X_test[:,:prefix,:],{'act_output':X_test[:,prefix,:10], 'time_output':X_test[:,prefix,10]})
print('Test set\n  Loss: {:0.3f}\n  act_output_acc: {:0.3f} \n  time_output_mean_absolute_error: {:0.3f}'.format(accr[0],accr[3],accr[4]))

# Suffix prediction and remaining cycle time prediction starts here
prefix =2

#only those processes who are not dead yet are taken, this filters the main X and only usable traces remain
X_train = X[X[:,prefix-1,0] == 0]
time = padded_time[X[:,prefix-1,0] == 0]

#data is divided into test and train
till = int(X_train.shape[0] *0.8)
X_test = X_train[till:]
X_train = X_train[:till]

time_test = time[till:]
time_train = time[:till]

#The weights corresponding to different prefixes are saved and loaded here
p2 = load_model('p2s1.h5')
p3 = load_model('p3s1.h5')
p4 = load_model('p4s1.h5')
p5 = load_model('p5s1.h5')

#we calculate the previous time because we have to add
# the new elapsed time to it to form the new timestamp
prevtime = time_test[:,prefix - 1]

pref = prefix #because the value of prefix will change so we want to store it in this variable

#prevtime and last_time are same but used for different purposes in the code
last_time = time_test[:,prefix-1]
# last_time

# We take the ground truth value of the future four events timestamp
#only four future is taken because in this dataset traces are of shorter length
# and our model predicts maximum of 4 events in the future with this data.
time_test = time_test[:,prefix:prefix+4]

#seq will contain future suffix of each example in test case
#time_pred will contain the time of the above said predicted suffix
seq = []
time_pred = []
for i in range(len(X_test)):
  seq.append([])
  time_pred.append([])
prefix=2
testX = X_test[:,:prefix,:]#we give the input according to the specified prefix
testnum = 1 # a random test number taken, so that we can follow what's happening to it

# the while loop below is for suffix and remaining cycle time prediciton
print("For instance we follow the test case with index 1 and see the predictions")
while(True):
  #pact is predicted activity in one-hot encoded form
  #ptime is predicted elapsed time in days.
  if (prefix == 2):
    pact,ptime=p2.predict(testX)
  elif (prefix == 3):
    pact,ptime=p3.predict(testX)
  elif (prefix == 4):
    pact,ptime=p4.predict(testX)
  elif (prefix == 5):
    pact,ptime=p5.predict(testX)
  else:
    #we predict only next 4 which is more than enough given the low number of events in the traces
    break
  preact = np.argmax(pact,axis = 1)
  #at this point for many traces end of trace i.e 0 would have occurred 
  #but we keep predicting forward and handle this issue afterwards in post-processing,
  # so the suffix might look like <4,5,0,5> which doesn't make sense but we clean it afterwards.
  newtime = []
  for i in range( len(ptime)):
    newtime.append( np.timedelta64( int((ptime*86400)[:,0][i] )  ,'s') + prevtime[i] )
  newtime = np.array(newtime)
  for i in range( len(ptime)):
    seq[i].append(preact[i])
    time_pred[i].append(newtime[i])
  print ( "\n"+str(testnum)+":Next activity is " + str(preact[testnum]) 
    + " performed at time "+ str(newtime[testnum]) + 
    " (previous activity time:" + str(prevtime[testnum]) + ")")

  #Now we extract information from the predicted data and concatenate it to our already given prefix to form
  #a new prefix to predict next activity and timestamp in the sequence.
  act = np.zeros(( len(ptime),10))
  prefix += 1
  for i in range( len(ptime)): 
    act[i,preact[i]] = 1
  act = act.reshape(-1,1,10)
  
  test = pd.DataFrame(newtime)
  mid = []
  sun = []
  elapsed = []
  for i in range( len(ptime)):
    mid.append( (test[0][i].hour*3600 + test[0][i].minute*60 + test[0][i].second)/84600 )
    sun.append( test[0][i].weekday() + mid[i] )
    elapsed.append( (test[0][i] - prevtime[i]).total_seconds()/86400 )
  mid = np.array(mid)
  sun = np.array(sun)
  elapsed = np.array(elapsed)
  mid = mid.reshape(-1,1,1)
  sun = sun.reshape(-1,1,1)
  elapsed = elapsed.reshape(-1,1,1)
  new = np.concatenate((act,mid,sun,elapsed),axis = 2 )
  X_new = np.concatenate ((testX,new) , axis = 1)
  
  prevtime = newtime
  testX = X_new
#loop ends



#For each trace zero i.e end of trace would have occurred in between,
# we note down the index of that using zerofirst, it notes down the fist occurance of zero
zerofirst = []
for li in seq:
  for i in range(len(li)):
    if (li[i] == 0 or i == len(li)-1):
      li[i:] = [0]*(len(li)- i)
      zerofirst.append(i)
      break

prefix = pref #retaining the original value of prefix from above
#we note down the ground truth suffix for comparison purpose
suffix = X_test[:,prefix:prefix+6-pref,:10]
#convert one-hot encoding to the actual value of the event i.e activity ID for comparion purposes
preact = np.argmax(suffix,axis = 2)

#similar to zerofirst but zerotest is for the ground truth suffix extracted earliar.
zerotest = []
for li in preact:
  for i in range(len(li)):
    if (li[i] == 0 or i == len(li)-1):
      li[i:] = [0]*(len(li)- i)
      zerotest.append(i)
      break
      
time_pred = np.array(time_pred)
#This is for remaining cycle time prediction, we calculate the last known timestamp from the ground truth suffix
i=0
last_rec = []
for li in time_test:
  index = zerotest[i]
  if (index == 0):
    last_rec.append(last_time[i])
    i+=1
    continue
  last_rec.append(li[index-1])
  i +=1

#This is for remaining cycle time prediction as well, 
#we calculate the last known timestamp from the predicted suffix

i=0
last_pred = []
for li in time_pred:
  index = zerofirst[i]
  if (index == 0  ):
    last_pred.append(last_time[i])
    i+=1
    continue
  last_pred.append(li[index-1])
  i +=1

#for the above two array created we find the difference in time 
diff = []

for i in range(len(last_pred)):
  if (last_pred[i] != 0):
    dt = last_pred[i]- last_rec[i]
    seconds = dt / np.timedelta64(1, 's')
    diff.append(abs(seconds/86400))

print(" the result of remaining time cycle prediciton in MAE is :"+ str(sum(diff)/len(diff)))

#for suffix prediction we have to convert them into string format
#for each suffix we find the string version with last element as zero
lpred = []
for li in seq:
  s = ""
  flag = 1
  for x in li:
    if (x !=0 ):
      s+=str(x)
  lpred.append(s)

#similar to above, we find the ground truth string version of suffix
ltruth = []
for li in preact:
  s = ""
  flag = 1
  for x in li:
    if (x !=0 ):
      s+=str(x)
  ltruth.append(s)

print("Installing strsim to calculate DLS")
pip install strsim

from similarity.damerau import Damerau
#below are two example of how it works
damerau = Damerau()
print(damerau.distance('ABCDEF', 'ABDCE'))
print(damerau.distance('ABCDE', 'ABCDE'))

# we calculate the DLS for each suffix and add the result to damdist array
damdist = []
for i in range(len(lpred)):
  if ( (max(len(ltruth[i]),len(lpred[i]))) == 0):
      damdist.append(0)
  else:
      dist = damerau.distance(ltruth[i], lpred[i])/(max(len(ltruth[i]),len(lpred[i])))
      damdist.append(dist)
  
  
print("The DLS value for suffix is :"+str(100-sum(damdist)/len(damdist)*100))

